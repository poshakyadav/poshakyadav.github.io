<main class="blog-main-container">
  <div class="blog-layout-wrapper">
    <article class="blog-article-content">
      <h1>Asynchronous Programming in Rust: A Comprehensive Guide</h1>

      <p>
        This document provides a detailed introduction to asynchronous programming in Rust, starting
        from foundational concepts like synchronous vs. asynchronous execution and concurrency. It
        covers Rust's async model, key components, examples, and best practices.
      </p>

      <h2>Basics of Programming Execution: Synchronous vs. Asynchronous</h2>

      <p>
        Before diving into Async/Await in Rust, let's start from the absolute basics. Imagine you're
        writing a program to perform tasks, like reading a file, making a network request (e.g.,
        fetching data from the internet), or doing some calculations. Programs can execute these
        tasks in different ways, and that's where <strong>synchronous</strong> and
        <strong>asynchronous</strong> come in.
      </p>

      <ul>
        <li>
          <strong>Synchronous Execution (Blocking)</strong>: This is the default way most simple
          programs work. Think of it like standing in line at a coffee shop. You order your coffee,
          and you wait right there until it's ready—no doing anything else. In code, if you have a
          task like "read a file," the program stops everything else until that task is done. This
          is straightforward but inefficient if the task takes time (e.g., waiting for a slow
          internet response). Your whole program is "blocked" during that wait.
        </li>
      </ul>

      <p>
        <strong>Detailed Example</strong>: In a synchronous program, if you're building a
        command-line tool that downloads a file and then processes it, the download blocks the
        entire execution. No other code runs until the download completes, which could lead to
        timeouts or unresponsive behavior in more complex apps.
      </p>

      <p><strong>Pros</strong>: Easy to reason about; code flows sequentially.</p>

      <p>
        <strong>Cons</strong>: Poor resource utilization; leads to "idle waiting" where the CPU is
        not doing useful work.
      </p>

      <ul>
        <li>
          <strong>Asynchronous Execution (Non-Blocking)</strong>: This is like ordering coffee and
          then stepping aside to check your phone or chat while it's being made. The barista calls
          you when it's ready. In code, you start a task (e.g., "start fetching data from the web")
          and continue with other work without waiting. When the task finishes, your program gets
          notified and handles the result. This allows your program to do multiple things "at the
          same time" without wasting time idling.
        </li>
      </ul>

      <p>
        <strong>Detailed Example</strong>: In an asynchronous web server, while waiting for a
        database query to return results (which might take 100ms due to network latency), the server
        can handle incoming requests from other clients. Once the query completes, a callback or
        notification resumes processing for that specific request.
      </p>

      <p>
        <strong>Pros</strong>: Better efficiency for waiting-heavy tasks; improves throughput in
        I/O-intensive applications.
      </p>

      <p>
        <strong>Cons</strong>: More complex code structure; requires handling callbacks, promises,
        or similar mechanisms to manage completion.
      </p>

      <p>
        Asynchronous programming is useful for I/O-bound tasks (things involving input/output, like
        file reading, network calls, or database queries) because these often involve waiting for
        external things (disk, network, etc.). It's less about CPU-heavy computations (like complex
        math), which might need other techniques like multithreading for parallelism.
      </p>

      <p><strong>I/O-Bound vs. CPU-Bound Tasks</strong></p>

      <ul>
        <li>
          <strong>I/O-Bound</strong>: Tasks limited by input/output speed (e.g., waiting for a hard
          drive to read data or a server to respond). Async shines here because it frees up the
          thread during waits.
        </li>
        <li>
          <strong>CPU-Bound</strong>: Tasks limited by processor speed (e.g., video encoding or
          machine learning training). These benefit more from parallelism via threads or distributed
          computing, as async doesn't inherently speed up computation—it just avoids blocking.
        </li>
      </ul>

      <h2>What is Concurrency?</h2>

      <p>Concurrency is a key concept tied to asynchronous programming. Let's break it down:</p>

      <ul>
        <li>
          <strong>Concurrency</strong>: This means handling multiple tasks at the same time, but not
          necessarily executing them simultaneously. It's like a single chef in a kitchen juggling
          multiple orders: chopping veggies for one while waiting for another to boil. The chef
          switches between tasks efficiently, making progress on all without finishing one
          completely before starting the next. In programming, concurrency allows your code to
          manage multiple operations (e.g., downloading files while processing user input) without
          blocking. It's about structuring your program to handle multiple operations efficiently,
          even if they don’t execute simultaneously.
        </li>
      </ul>

      <ul>
        <li>
          In programming, concurrency is useful for tasks like handling user inputs, processing
          network requests, or reading files, where waiting for one task shouldn’t stop the entire
          program.
        </li>
      </ul>

      <ul>
        <li>
          <strong>Parallelism</strong>: This is often confused with concurrency, but it's different.
          Parallelism is true simultaneous execution, like multiple chefs working on different
          orders at the exact same time. It requires multiple CPU cores (hardware). Concurrency can
          happen on a single core by switching tasks quickly (called "context switching").
        </li>
      </ul>

      <h3>Threads for Parallelism</h3>

      <p>Rust supports concurrency through threads, which are a way to achieve parallelism.</p>

      <ul>
        <li>
          <strong>What are Threads?</strong> A thread is a unit of execution within a program. When
          you create multiple threads, the operating system can run them on different CPU cores,
          allowing tasks to execute simultaneously—this is parallelism.
        </li>
      </ul>

      <ul>
        <li>
          <strong>Parallelism vs. Concurrency:</strong>
          <ul>
            <li>
              <strong>Parallelism</strong> means tasks are running at the exact same time, typically
              on multiple CPU cores. For example, if you have a four-core CPU, you could run four
              threads simultaneously, each performing a different computation.
            </li>
            <li>
              <strong>Concurrency</strong> is broader and doesn’t require simultaneous execution.
              It’s about managing multiple tasks, even if they take turns running on a single core.
            </li>
            <li>
              <strong>Analogy:</strong> Parallelism is like multiple chefs, each working on their
              own dish at the same time in the same kitchen. Concurrency might involve one chef
              juggling multiple dishes by switching between them.
            </li>
          </ul>
        </li>
      </ul>

      <ul>
        <li>
          <strong>Rust’s Thread Support:</strong>
          <ul>
            <li>
              Rust has built-in support for threads in its standard library (std::thread). You can
              create threads to run tasks in parallel, which is ideal for CPU-bound tasks (e.g.,
              heavy computations like image processing or mathematical calculations).
            </li>
            <li>
              Example: If you’re processing a large dataset, you could split it into chunks and
              assign each chunk to a separate thread. Each thread runs on a different CPU core,
              speeding up the process.
            </li>
            <li>
              Rust’s ownership and borrowing rules ensure thread safety, preventing issues like data
              races (where multiple threads access shared data unsafely).
            </li>
          </ul>
        </li>
      </ul>

      <p>In summary:</p>

      <ul>
        <li>
          Concurrency: Multiple tasks making progress over the same period, possibly by
          interleaving.
        </li>
        <li>
          Parallelism: Multiple tasks running literally at the same instant on different processors.
        </li>
      </ul>

      <table>
        <thead>
          <tr>
            <th>Concept</th>
            <th>Analogy</th>
            <th>Key Trait</th>
            <th>Requires Multiple CPUs?</th>
            <th>Example Use Case</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Concurrency</td>
            <td>One juggler with multiple balls</td>
            <td>Task switching/interleaving</td>
            <td>No</td>
            <td>Web server handling multiple client requests on one thread</td>
          </tr>
          <tr>
            <td>Parallelism</td>
            <td>Multiple jugglers, each with one ball</td>
            <td>True simultaneity</td>
            <td>Yes</td>
            <td>Matrix multiplication split across CPU cores</td>
          </tr>
        </tbody>
      </table>

      <p>
        Concurrency is about <em>design</em> (how your code is structured to handle multiple tasks),
        while parallelism is about <em>execution</em> (how the hardware runs it). Asynchronous
        programming is one way to achieve concurrency.
      </p>

      <h2>Why Do We Need This in Programming?</h2>

      <p>
        In real-world apps (e.g., web servers, games, or GUIs), programs often deal with slow
        operations:
      </p>

      <ul>
        <li>
          Waiting for user input (e.g., in a GUI, async prevents the UI from freezing while loading
          data).
        </li>
        <li>
          Fetching data over the network (which can take seconds due to latency, bandwidth, or
          server load).
        </li>
        <li>
          Reading/writing files (disk I/O can be slow on mechanical drives or when handling large
          files).
        </li>
      </ul>

      <p>
        Without concurrency, your app would freeze during these waits, leading to a bad user
        experience. Concurrency keeps things responsive.
      </p>

      <p>
        <strong>Detailed Example</strong>: In a mobile app, synchronous network calls would make the
        app unresponsive during poor connectivity. Async allows the user to scroll or interact while
        data loads in the background.
      </p>

      <p>
        <strong>Scalability Benefits</strong><br />
        Concurrency enables handling thousands of connections (e.g., in servers like Nginx or Rust's
        Hyper) with low resource usage. Without it, you'd need one thread per connection, which is
        memory-intensive (each thread might use 2-8 MB of stack space).
      </p>

      <h2>Asynchronous Programming in Rust</h2>

      <p>
        Rust is a systems programming language focused on safety, performance, and concurrency. Rust
        provides built-in mechanisms to handle concurrency, enabling efficient multitasking in
        programs. It supports concurrency through two main approaches: (1) <strong>threads</strong>,
        which allow multiple tasks to run simultaneously on different CPU cores (achieving
        parallelism), and (2) <strong>asynchronous programming</strong>, which enables tasks like
        input/output operations (e.g., reading files or fetching data from the internet) to run
        without blocking the program’s execution, improving responsiveness.
      </p>

      <p><strong>Detailed Explanation of Rust's Concurrency Support</strong>:</p>

      <ul>
        <li>
          <strong>Threads for Parallelism</strong>: Rust's standard library includes
          <code class="code-highlight">std::thread</code>, allowing you to spawn OS threads for parallel execution. This is
          ideal for CPU-bound tasks where you can divide work across cores. For example, in a video
          processing app, threads can encode different frames simultaneously. Rust's ownership model
          prevents data races by ensuring data is either owned by one thread or shared
          immutably/mutably with safeguards like <code class="code-highlight">Mutex</code> or <code class="code-highlight">Arc</code>.<br />
          <strong>Example Code with Line-by-Line Explanation</strong>:
        </li>
      </ul>

      <app-code-block [code]="codeExOne" [language]="'rust'" [id]="'codeExOne'"></app-code-block>

      <ul>
        <li>Line 1: Imports necessary for threading.</li>
        <li>Line 3: Starts the main function.</li>
        <li>
          Line 4: <code class="code-highlight">thread::spawn</code> creates a new OS thread and runs the closure
          asynchronously (but in parallel if cores are available).
        </li>
        <li>Lines 5-7: Thread-local computation; no blocking of main.</li>
        <li>Line 10: Main prints while the thread computes.</li>
        <li>
          Line 12: <code class="code-highlight">join()</code> blocks main until the thread finishes, unwrapping the
          result.<br />
          This achieves parallelism: on a multi-core CPU, the loop runs simultaneously with main's
          "other work."
        </li>
      </ul>

      <ul>
        <li>
          <strong>Asynchronous Programming for Non-Blocking I/O</strong>: Rust uses futures and
          async/await for non-blocking operations. This is lightweight and suited for I/O-bound
          tasks, where threads would be overkill due to context-switching overhead. For example, in
          a chat app, async handles multiple socket reads without dedicating a thread per user.
        </li>
      </ul>

      <p>
        Rust's async model is based on <strong>futures</strong>. A future is like a "promise" of a
        value that will be available later (e.g., the data from a network request).
      </p>

      <p>
        <strong>Detailed Explanation of Futures</strong>: A <code class="code-highlight">Future</code> is a trait in
        <code class="code-highlight">std::future</code> with a <code class="code-highlight">poll</code> method. When polled, it returns
        <code class="code-highlight">Poll::Pending</code> if not ready or <code class="code-highlight">Poll::Ready(value)</code> if complete. This
        allows efficient checking without constant spinning.
      </p>

      <ul>
        <li>
          <strong>Without Async/Await</strong>: Early async code in Rust used callbacks or manual
          polling of futures, which was messy and hard to read (callback hell). For example, you'd
          chain <code class="code-highlight">.then()</code> methods, leading to nested code.
        </li>
      </ul>

      <p><strong>Example of Callback Hell</strong>:</p>

      <app-code-block [code]="codeExTwo" [language]="'rust'" [id]="'codeExTwo'"></app-code-block>

      <p>This pyramids indentation and is error-prone.</p>

      <ul>
        <li>
          <strong>With Async/Await</strong>: Introduced in Rust 1.39 (2019), this syntax makes async
          code look almost like synchronous code, but it's non-blocking under the hood. It desugars
          to efficient state machines.
        </li>
      </ul>

      <p>
        Rust doesn’t have a built-in async runtime in its core language (unlike JavaScript or
        Python). Instead, it relies on external libraries (called crates) like <code class="code-highlight">tokio</code> or
        <code class="code-highlight">async-std</code> to provide the "runtime" that schedules and runs async tasks. These
        runtimes use an event loop (a loop that checks for completed tasks and runs their
        continuations).
      </p>

      <p><strong>Key Components in Rust Async</strong>:</p>

      <ul>
        <li>
          <strong>Future</strong>: A trait representing an async computation. It has a
          <code class="code-highlight">poll</code> method that checks if it's ready. Polling drives progress; the executor
          calls it repeatedly. For asynchronous programming, Rust’s <code class="code-highlight">Future</code> trait (part
          of the standard library) defines how async computations work. However, you typically use
          libraries like <code class="code-highlight">tokio</code> or <code class="code-highlight">async-std</code> to run these futures.
        </li>
      </ul>

      <ul>
        <li>
          <strong>Executor</strong>: Part of the runtime (e.g., Tokio's executor) that runs futures
          to completion by polling them.
        </li>
      </ul>

      <p>
        <strong>Detailed Explanation of Executor</strong>: An executor is the "engine" that manages
        async tasks. It maintains a queue of futures, polls them when possible (e.g., when I/O is
        ready), and handles waking (notification when a future can progress). Unlike threads,
        executors are user-space and lightweight.
      </p>

      <ul>
        <li>
          <strong>How It Works</strong>: The executor runs an event loop, registering interest in
          events (e.g., via epoll on Linux for I/O readiness). When an event occurs, it wakes the
          corresponding future and polls it.
        </li>
        <li>
          <strong>Tokio Executor Example</strong>: Tokio can be single-threaded (for pure
          concurrency) or multi-threaded (for concurrency + parallelism, e.g., via
          <code class="code-highlight">#[tokio::main(flavor = "multi_thread")]</code>). In multi-threaded mode, it uses a
          work-stealing scheduler to distribute tasks across threads.
        </li>
        <li>
          <strong>Why Important</strong>: Without an executor, futures don't run—they're lazy. You
          must "drive" them with an executor like <code class="code-highlight">tokio::runtime::Runtime</code>.
        </li>
      </ul>

      <p>
        <strong>Example with Custom Executor Setup</strong> (beyond <code class="code-highlight">#[tokio::main]</code> for
        illustration):
      </p>

      <app-code-block
        [code]="codeExThree"
        [language]="'rust'"
        [id]="'codeExThree'"
      ></app-code-block>

      <ul>
        <li>Line 1-2: Imports.</li>
        <li>Line 4-6: Defines an async function; <code class="code-highlight">.await</code> yields to executor.</li>
        <li>Line 8: Creates executor.</li>
        <li>
          Line 9: <code class="code-highlight">block_on</code> runs the async closure to completion, polling futures
          inside.<br />
          This shows manual control; useful for embedding async in sync code.
        </li>
      </ul>

      <ul>
        <li>
          <strong>Async Functions</strong>: The <code class="code-highlight">async</code> keyword marks a function as
          asynchronous that returns a <code class="code-highlight">Future</code> and the <code class="code-highlight">.await</code> operator
          pauses execution of an async function until a future is ready, without blocking the entire
          thread.
        </li>
      </ul>

      <ul>
        <li>
          <strong>Await</strong>: The <code class="code-highlight">.await</code> operator pauses the current async function
          until the future is ready, without blocking the thread. It yields control back to the
          executor.
        </li>
      </ul>

      <p>
        Rust's async is "zero-cost" – it doesn't add runtime overhead unless you use it, and it's
        efficient because it compiles to state machines (under the hood, async functions are turned
        into efficient code that tracks progress).
      </p>

      <p>
        <strong>Detailed Explanation of "Zero-Cost"</strong>: "Zero-cost abstractions" mean features
        that don't impose performance penalties at runtime compared to hand-written low-level code.
        For async, this means no extra allocations or indirections unless necessary.
      </p>

      <ul>
        <li>
          <strong>How It Achieves Zero-Cost</strong>: Async functions compile to state machines
          (enums representing states like "waiting for I/O" or "processing result"). Each
          <code class="code-highlight">.await</code> becomes a state transition. No garbage collection or virtual
          calls—pure stack-based efficiency.
        </li>
        <li>
          <strong>Example Comparison</strong>: In languages like Go, goroutines have runtime
          overhead (small stacks, but still allocations). In Rust, a simple async function might
          compile to code as efficient as a manual polling loop.
        </li>
        <li>
          <strong>Benchmark Example</strong>: An async HTTP server in Rust (using Hyper/Tokio) can
          handle 100k requests/sec with minimal CPU, whereas a threaded server might use more
          memory. If you don't use async, your binary has no extra code or data.
        </li>
        <li>
          <strong>Trade-Off</strong>: Compile-time complexity increases (longer build times for
          large async codebases), but runtime is optimal.
        </li>
      </ul>

      <p><strong>Why Use Threads vs. Async?</strong></p>

      <ul>
        <li>
          <strong>Threads (Parallelism):</strong>
          <ul>
            <li>
              Best for CPU-bound tasks where you need to perform heavy computations and can benefit
              from multiple CPU cores.
            </li>
            <li>
              Example: Rendering a 3D scene in a game, where each thread processes part of the
              image.
            </li>
            <li>
              Drawback: Threads are “heavier” because each thread requires its own stack and
              operating system resources. Creating thousands of threads can be inefficient.
            </li>
          </ul>
        </li>
      </ul>

      <ul>
        <li>
          <strong>Async (Non-Blocking I/O):</strong>
          <ul>
            <li>
              Best for I/O-bound tasks where the program spends time waiting for external resources
              (e.g., network, disk).
            </li>
            <li>
              Example: A web server handling thousands of client connections, where each connection
              involves waiting for requests and responses.
            </li>
            <li>
              Advantage: Async tasks are lightweight because they run on a single thread (or a few
              threads) managed by a runtime like tokio. This allows handling thousands of tasks with
              minimal overhead.
            </li>
          </ul>
        </li>
      </ul>

      <p>
        <strong>Putting It All Together</strong> Rust’s concurrency model gives you two powerful
        tools:
      </p>

      <ul>
        <li>
          <strong>Threads</strong> for parallelism, where tasks run simultaneously on multiple CPU
          cores, ideal for compute-heavy work.
        </li>
        <li>
          <strong>Asynchronous programming</strong> for non-blocking I/O, where tasks like network
          requests or file operations can run without halting the program, ideal for responsive
          applications like servers or GUIs.
        </li>
      </ul>

      <h2>Detailed Explanation of Async/Await in Rust</h2>

      <p>
        Let's assume you're using the <code class="code-highlight">tokio</code> crate, which is popular for async Rust.
        First, add it to your <code class="code-highlight">Cargo.toml</code> (enable the 2024 edition for modern features
        like async closures):
      </p>

      
      <app-code-block [code]="codeExFour" [language]="'rust'" [id]="'codeExFour'"></app-code-block>

      <h4>1. Writing an Async Function</h4>

      <p>
        An async function is declared with <code class="code-highlight">async fn</code>. It returns a type that implements
        the <code class="code-highlight">Future</code> trait.
      </p>

      <p>Example (synchronous version first for comparison):</p>

      
      <app-code-block [code]="codeExFive" [language]="'rust'" [id]="'codeExFive'"></app-code-block>

      <ul>
        <li>Line 2: Defines sync function.</li>
        <li>Line 4: Blocks current thread; no other work happens.</li>
        <li>Line 5: Returns string.</li>
        <li>
          Line 8: Calls function, blocks.<br />
          This is simple but inefficient for I/O.
        </li>
      </ul>

      <p>Now, async version:</p>

      <app-code-block [code]="codeExSix" [language]="'rust'" [id]="'codeExSix'"></app-code-block>

      <ul>
        <li>Line 1: Imports.</li>
        <li>Line 4: <code class="code-highlight">async fn</code> makes it return a Future.</li>
        <li>
          Line 6: <code class="code-highlight">sleep</code> returns a Future; <code class="code-highlight">.await</code> suspends this function
          until ready.
        </li>
        <li>Line 7: Executes post-await.</li>
      </ul>

      <h4>2. Running Async Code</h4>

      <p>
        You can't call async functions directly in <code class="code-highlight">main</code> (which is sync). You need an
        executor.
      </p>

      <app-code-block
        [code]="codeExSeven"
        [language]="'rust'"
        [id]="'codeExSeven'"
      ></app-code-block>

      <ul>
        <li>Line 1: Macro sets up executor and makes main async.</li>
        <li>Line 3: Calls async function and awaits.</li>
      </ul>

      <h4>3. Concurrency with Multiple Async Tasks</h4>

      <p>To show concurrency, let's run multiple fetches "at the same time."</p>

      <app-code-block
        [code]="codeExEight"
        [language]="'rust'"
        [id]="'codeExEight'"
      ></app-code-block>

      <ul>
        <li>Lines 3-6: fetch_a with 3s wait.</li>
        <li>Lines 8-11: fetch_b with 2s.</li>
        <li>Line 15-16: <code class="code-highlight">tokio::spawn</code> schedules tasks on the executor.</li>
        <li>
          Line 19-20: Await joins them; total runtime ~3s due to overlap.<br />
          This demonstrates concurrency: tasks interleave on one thread.
        </li>
      </ul>

      <p>
        <strong>Additional Example: Using <code class="code-highlight">join!</code> Macro for Simpler Concurrency</strong>:
      </p>

      <app-code-block [code]="codeExNine" [language]="'rust'" [id]="'codeExNine'"></app-code-block>

      <ul>
        <li>Line 8: <code class="code-highlight">join!</code> macro awaits all, handling concurrency automatically.</li>
      </ul>

      <h4>4. Error Handling in Async</h4>

      <p>
        Async functions can return <code class="code-highlight">Result</code> for errors. For more ergonomic error handling
        in real apps, consider crates like <code class="code-highlight">anyhow</code> or <code class="code-highlight">thiserror</code> for
        propagation.
      </p>
      
      <app-code-block [code]="codeExTen" [language]="'rust'" [id]="'codeExTen'"></app-code-block>

      <ul>
        <li>Line 3: Async with Result.</li>
        <li>
          Line 11: Await and handle errors.<br />
          Use <code class="code-highlight">?</code> for propagation: <code class="code-highlight">let data = maybe_fetch().await?;</code> in async
          contexts.
        </li>
      </ul>

      <h4>5. Real-World Example: Async HTTP Request</h4>

      <p>
        Using <code class="code-highlight">reqwest</code> crate for network (add
        <code id="codeHighlightedTextTwo" class="code-highlight" [innerText]="codeHighlightedTextTwo"></code> to
        Cargo.toml). For JSON responses, you can use
        <code class="code-highlight">.json::&lt;Type&gt;().await?</code> directly for parsing.
      </p>

      <app-code-block
        [code]="codeExEleven"
        [language]="'rust'"
        [id]="'codeExEleven'"
      ></app-code-block>

      <ul>
        <li>Line 4: <code class="code-highlight">get</code> initiates request.</li>
        <li>Line 5: Awaits status/code/headers.</li>
        <li>
          Line 6-7: Awaits full body.<br />
          This is non-blocking; multiple <code class="code-highlight">get</code>s can run concurrently.
        </li>
      </ul>

      <p><strong>Additional Real-World Example: Concurrent Downloads</strong>:</p>

      <p>
        To demonstrate true concurrency in HTTP requests, we use a shared
        <code class="code-highlight">reqwest::Client</code> and <code class="code-highlight">futures::future::join_all</code>. Add
        <code class="code-highlight">futures = "0.3.31"</code> to your <code class="code-highlight">Cargo.toml</code>.
      </p>

      <p>
        The <code class="code-highlight">Client</code> is used for connection pooling—sharing connections across requests
        to improve performance by reusing TCP connections instead of establishing new ones each
        time. Since <code class="code-highlight">Client</code> implements <code class="code-highlight">Clone</code>, cloning it is cheap;
        internally, it uses smart pointers like <code class="code-highlight">Arc</code> (Atomic Reference Counting) to
        share the underlying state safely across multiple futures or tasks without ownership
        conflicts. <code class="code-highlight">Arc</code> allows multiple owners of the same data, incrementing a
        reference count atomically for thread-safety.
      </p>

      <p>
        <code class="code-highlight">join_all</code> takes an iterator of futures and runs them concurrently on the
        executor. Each future starts its HTTP request (<code class="code-highlight">send().await?</code>) independently,
        allowing the network operations to overlap. This reduces total latency: if each request
        takes 1 second, two concurrent requests complete in ~1 second total, not 2 seconds. The
        <code class="code-highlight">.await</code> points are inside the async blocks, so the requests begin as soon as the
        futures are created and polled by the executor.
      </p>
      
      <app-code-block
        [code]="codeExTwelve"
        [language]="'rust'"
        [id]="'codeExTwelve'"
      ></app-code-block>

      <ul>
        <li>
          This pattern is scalable for many URLs. Without the shared <code class="code-highlight">Client</code> and
          deferred <code class="code-highlight">.await</code>, requests would run sequentially, as immediate
          <code class="code-highlight">.await</code> on each <code class="code-highlight">get</code> would block until completion before starting
          the next.
        </li>
      </ul>

      <h2>Related Concepts</h2>

      <ul>
        <li>
          <strong>Futures Crate</strong>: The foundation. All async relies on
          <code class="code-highlight">std::future::Future</code>. It's poll-based for efficiency.
        </li>
      </ul>

      <ul>
        <li>
          <strong>Pin and Unpin</strong>: Async code often needs "pinning" (fixing in memory)
          because futures can be self-referential.
        </li>
      </ul>

      <p>
        <strong>Detailed Explanation</strong>: Futures might hold references across await points, so
        moving them in memory could invalidate pointers. <code class="code-highlight">Pin</code> ensures a value stays at a
        fixed address. Most users don't deal with it directly—<code class="code-highlight">async</code> blocks handle
        pinning via <code class="code-highlight">Box::pin</code> if needed. This is crucial for generators (async's
        internal representation).
      </p>

      <p><strong>Example</strong>:</p>

      <app-code-block
        [code]="codeExThirteen"
        [language]="'rust'"
        [id]="'codeExThirteen'"
      ></app-code-block>

      <ul>
        <li>
          <strong>Streams</strong>: Like futures but for multiple values (e.g., async iterators).
          From <code class="code-highlight">futures</code> crate. Note: As per the Async Book (2025 rewrite), streams have
          some rough edges and are evolving.
        </li>
      </ul>

      <p><strong>Example</strong>:</p>
      
      <app-code-block
        [code]="codeExFourteen"
        [language]="'rust'"
        [id]="'codeExFourteen'"
      ></app-code-block>

      <ul>
        <li>Useful for processing data chunks, like reading a file line-by-line async.</li>
      </ul>

      <ul>
        <li>
          <strong>Async Fn in Traits</strong>: Stabilized in Rust 1.75 (2023), allowing async
          methods in traits without workarounds. This enables better async APIs, e.g., in frameworks
          like Axum.
        </li>
      </ul>

      <ul>
        <li>
          <strong>Runtimes</strong>: Tokio (full-featured, multi-threaded, supports
          timers/FS/network), async-std (std-like API, simpler), smol (lightweight, minimal deps).
        </li>
      </ul>

      <p><strong>Comparison</strong>:</p>

      <table>
        <thead>
          <tr>
            <th>Runtime</th>
            <th>Strengths</th>
            <th>Use Case</th>
            <th>Multi-Threaded Support</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Tokio</td>
            <td>Rich ecosystem, multi-thread</td>
            <td>Servers, complex apps</td>
            <td>Yes</td>
          </tr>
          <tr>
            <td>async-std</td>
            <td>Familiar std APIs</td>
            <td>Scripts, simple async</td>
            <td>Limited</td>
          </tr>
          <tr>
            <td>smol</td>
            <td>Small footprint</td>
            <td>Embedded systems</td>
            <td>No</td>
          </tr>
        </tbody>
      </table>

      <ul>
        <li>
          <strong>Blocking in Async</strong>: Avoid sync blocking calls in async code (e.g., don't
          use <code class="code-highlight">std::thread::sleep</code> in async fn—it blocks the executor). Use async
          equivalents like <code class="code-highlight">tokio::time::sleep</code>.
        </li>
      </ul>

      <ul>
        <li>
          <strong>Thread Safety</strong>: Rust's ownership ensures safety, but async adds complexity
          with <code class="code-highlight">Send</code> and <code class="code-highlight">Sync</code> traits for cross-thread futures.
          <code class="code-highlight">Send</code> means a type can be sent to another thread; required for spawning tasks
          in multi-threaded executors.
        </li>
      </ul>

      <p>
        <strong>Additional Point: Integrating Async with Threads</strong><br />
        You can mix them for hybrid concurrency (e.g., async for I/O, threads for CPU). Use
        <code class="code-highlight">tokio::task::spawn_blocking</code> for sync code in async.
      </p>

      <p><strong>Example</strong>:</p>
      
      <app-code-block
        [code]="codeExFifteen"
        [language]="'rust'"
        [id]="'codeExFifteen'"
      ></app-code-block>

      <ul>
        <li>This offloads blocking work, keeping the async executor free.</li>
      </ul>

      <p>
        <strong>New in Rust 2024 Edition: Async Closures</strong><br />
        Stabilized in Rust 1.85 (February 2025), async closures (e.g.,
        <code id="codeHighlightedTextOne" class="code-highlight" [innerText]="codeHighlightedTextOne"></code>) capture
        variables and return futures. This simplifies code in higher-order functions like
        <code class="code-highlight">map</code> or custom executors, reducing boilerplate.
      </p>

      <p><strong>Example</strong>:</p>
      
      <app-code-block
        [code]="codeExSixteen"
        [language]="'rust'"
        [id]="'codeExSixteen'"
      ></app-code-block>
      <h2>Common Pitfalls for Beginners</h2>

      <ul>
        <li>
          Forgetting to <code class="code-highlight">.await</code> a future: It won't run! The future is created but not
          driven.
        </li>
        <li>
          Mixing sync and async: Use <code class="code-highlight">tokio::task::block_in_place</code> if you must call sync
          code in async, but prefer async alternatives.
        </li>
        <li>
          Borrow Checker Issues: Async can lead to lifetime errors because futures hold references
          across awaits. Use <code class="code-highlight">'static</code> or <code class="code-highlight">Arc</code> for shared data.
        </li>
        <li>
          Performance: Async is great for I/O, but for CPU-bound, use threads (e.g.,
          <code class="code-highlight">std::thread</code> or Rayon crate for data parallelism).
        </li>
        <li>
          Cancellation: Rust futures don't support built-in cancellation; you must design for it
          (e.g., using channels to signal drop).
        </li>
        <li>
          <strong>Unrestrained Cooperation</strong>: In cooperative scheduling, long-running async
          tasks can starve others. Use <code class="code-highlight">tokio::task::yield_now().await</code> for fairness in
          compute-heavy async code.
        </li>
      </ul>

      <p>
        <strong>Additional Pitfall: Deadlocks in Multi-Threaded Async</strong><br />
        In Tokio's multi-threaded mode, awaiting a future that isn't <code class="code-highlight">Send</code> causes
        compile errors. Always ensure cross-thread safety.
      </p>

      <h2>Practice Tips</h2>

      <ul>
        <li>
          Start with simple examples in a new Cargo project:
          <code class="code-highlight">cargo new async_demo &amp;&amp; cd async_demo</code>.
        </li>
        <li>
          Read Rust's async book: <a href="https://rust-lang.github.io/async-book/">https://rust-lang.github.io/async-book/</a> (note: undergoing rewrite
          in 2025, some sections incomplete).
        </li>
        <li>
          Experiment: Try making a concurrent downloader for multiple URLs using
          <code class="code-highlight">reqwest</code> and <code class="code-highlight">tokio::fs</code> for saving files.
        </li>
        <li>Tools: Use <code class="code-highlight">cargo watch</code> for hot-reloading during development.</li>
        <li>
          Advanced: Explore actor models with <code class="code-highlight">actix</code> or web frameworks like
          <code class="code-highlight">axum</code> for real async apps. Enable the 2024 edition in
          <code class="code-highlight">Cargo.toml</code> for features like async closures. Use Tokio Console for debugging
          async tasks: <code class="code-highlight">cargo install tokio-console</code>.
        </li>
      </ul>
    </article>
  </div>
</main>
